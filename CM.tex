\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\author{Malo Kerebel}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black, urlcolor=black
}

%Macro
\newcommand{\ens}[1]{\mathbb{#1}}

\begin{document}


\begin{titlepage}

\centering{
	
	{\scshape\LARGE Université de Bretagne Occidentale \par}
	\vspace{1cm}
	{\scshape\Large Note de cours\par}
	\vspace{1.5cm}
	{\huge\bfseries Physique statistique\unskip\strut\par}
	\vspace{2cm}
	{\Large\itshape Malo Kerebel \par}
	\vfill
	Cours par\par
	Jean-Philippe \textsc{Jay}

	\vfill

% Bottom of the page
	{\large Semestre 6, année 2020-2021 \par}
}
\end{titlepage}

\tableofcontents

\chapter{Introduction générale}
\begin{center}
\textbf{CM1 (2021-01-12)}
\end{center}

physique statistique : étude du mouvement de gaz à l'échelle macroscopique.
Différences entre fermion et boson : leur spin, fermion spin demi entier (eg. les électrons), boson particule à spin entier (eg photon)\par
\quad \par
Bibliographie:
Physique Statistique, B. Diu, C. uthmann, D.Lederer\\
Physique Statistique, H. Ngp, C. Ngo\par
\quad \par

\section{Buts de la physique statistique}
Unifier le macroscopique et le microscopique, au $\text{XIV}^{\text{\`eme}}$ siècle, on a la thermodynamique, la mécanique et l'électro-magnétisme mais rien qui relie les uns aux autres.

Au niveau microscopique on a $\approx 10^{23}$ paramètres (de l'ordre du nombre d'Avogadro), auquel il faut avoir la vitesse et la position, il est impossible d'appliquer les résultats de la mécanique macroscopique dessus.
La physique statistique a donc pour but d'expliquer les comportements collectifs, de particules mais les résultats peuvent s'étendre à des réseaux de neurones ou des comportements de foules.

\section{Combinatoire}
Le dénombrement des objets ou des configuration. 2 système indépendant A et B, ayant $\Omega_a$ et $\Omega_b$ configurations, il y a \(\Omega_a \times \Omega_b\) configurations possible pour la juxtaposition de A et B.

\begin{center}
\textbf{CM2 (2021-01-14)}
\end{center}

Le nombre de permutations de N objets parmi N est N!

Le nombre de combinaison sans répétition s'obtient avec :
\[
	C^k_n = \dfrac{n!}{k!(n-k)!}
\]

\section{Probabilités - Statistiques}

événement aléatoire : résultat possible d'une expérience

Variable aléatoire : variable qui peut prendre l'une quelconque de ses valeurs possibles, inconnue d'avance. Discrète il y a un nombre finie de valeur, continue il y a un nombre infinie de valeur possible

\paragraph{Propriétés}
\begin{enumerate}
	\item \(0 \leq P_m \leq 1 \quad \forall m\)
	\item \(\sum_m P_m = 1\) (normalisation)
\end{enumerate}

Pour une variable aléatoire continue on utilise la densité de probabilité, qu'une la variable \(\in [x, x +\delta x]\)
\[
	w(x) = \lim_{\delta_x \rightarrow 0^+} \frac{\delta P(x)}{\delta_x}
\]
\[
	dP(x) = w(x)dx = \lim_{N \rightarrow \infty} \frac{dN(x)}{N}
\]
De même il y a la normalisation :
\[
	\int_{-\infty}^{+\infty} w(x)dx = 1
\]

Quand on est dans les bonne conditions :
\[
	P(e_1 ou e_2) = P(e_1) + P(e_2)
\]
\[
	P(e_1 et e_2) = P(e_1)\cdot P(e_2)
\]

L'écart quadratique moyen, ou variance, caratérise la dispersion de la distribution statistique, il est défini par :
\[
	(\Delta_f)^2 = \overline{(f - \overline{f})^2} = \overline{f^2} - (\overline{f})^2
\]
De même on définit l'écart-type \(\sigma\) comme la racine carré de la variance :
\[
	\sigma = \sqrt{(\Delta_f)^2} = \Delta_f
\]

La moyenne de résultat est :
\[
	\overline{n} = pN
\]

\begin{center}
\textbf{CM3 (2021-01-19)}
\end{center}

Après calculs : \(\overline{n^2} = pN + p^2N(N-1)\)
Donc \(\sigma^2 = \overline{n^2} - (\overline{n})^2 = pN + p^2N^2 - p^N - p^2N^2 = pN - p^2N\)
\[
	\sigma^2 = N p q = Np(1-p)
\]

Ainsi on voit que l'écart type croit avec N :
(Cependant, la dispersion relative à la moyenne décroit)
\[
	\dfrac{\sigma}{\overline{n}} \sim \dfrac{\sqrt{N}}{N} = \dfrac{1}{\sqrt{N}}
\]

\subsection{Loi de Poisson}

Hypothèses :
\begin{itemize}
	\item[•] \(p \rightarrow 0\)
	\item[•] \(N \rightarrow +\infty\)
	\item[•] MAIS \(pN = \lambda\) reste une valeur finie.
\end{itemize}

La loi de poisson dit donc que la loi binomiale devient :
\[
	P(N, n) = \dfrac{\lambda^n}{n!}e^{-\lambda}
\]
La proba ne dépend pas du nombre d'expériences N
Exercice : Vérifier que c'est bien normalisé.
\[
	\sum_n P(N,n) = 1 = \sum_n \dfrac{\lambda^n}{n!}e^{-\lambda} = e^\lambda e^{-\lambda} = 1
\]

On a :
\[
	\overline{n} = \sigma^2 = \lambda
\]
On peut donc aussi écrire la loi de poisson sous la forme :
\[
	P = \dfrac{\overline{n}^n}{n!}e^{-\overline{n}}
\]

\subsection{Loi Gaussienne ou loi normale}

La densité de probabilité est :
\[
	w_G (c) = \dfrac{1}{\sqrt{2\pi \sigma^2}} e^{\left[ \dfrac{- (x-x_0)^2}{2\sigma^2} \right]}
\]
\(w_G\) dépend de 2 paramètres, le centre (ou la moyenne) \(x_0\) et la largeur \(\sigma\)

\subsection{Théorème de la limite centrale}

La distribution de la somme de N variables aléatoires indépendantes et de même loi devient gaussienne lorsque N devient grand

En particulier la loi binomiale \(\rightarrow\) loi gaussienne

Théorème important en physique stats, car les systèmes macroscopiques sont souvent constitué d'un très grand nombre d'éléments faiblement couplés.

Les variables libres de fluctuer ont alors une distribution gaussienne.

\section{Entropie statistique}

Les informations concernant le ssystème physique est incomplète
L'entropie statistique mesure le manque d'information

\subsection{Définition de l'entropie statistique}

Ensemble fini d'événements \(e_m (m = 1,2, \cdots , M)\) de proba \(P_m\).
Tout autre distribution implique un manque d'information
ENtropie statistique :
\[
	S(P_1, P_2, \cdots, P_M) = -k \sum_{m=1}^M P_m \ln(P_m)
\]
Avec \(P_m \ln(P_m) = 0\) si \(P_m = 0\) et \(k = k_B = 1,380649 \times 10^{-23} J\cdot K^{-1}\) la constante de Boltzmann, seule l'ordre de grandeur et l'unité sont à retenir.

L'entropie est nulle si une des proba des évenement possible est de 1. Inversement l'entropie est maximale quand tous les évenements sont équiprobable. L'entropie augmente avec le nombre d'évenement possible.

\section{Quelques résultat de maths}

\subsection{Approximation de Stirling}

Approximation utile pour plus tard :
Pour \(N \gg 1\), on a :
\[
	\ln(N!) \sim N \ln(N) - N
\]

\subsection{Intégrales gaussiennes}

\[
	\int_{-\infty}^{+\infty} e^{-\alpha x^2}dx = \sqrt{\dfrac{\pi}{\alpha}}
\]

\subsection{Méthode des multiplicateurs de Lagrange}

f fonction de n variables indépendantes. Il y a un maxima si :
\[
	\dfrac{\partial f}{\partial x_1} = \cdots = \dfrac{\partial f}{\partial x_n} = 0
\]

Si il y a une contraintes entre les variables, ex \(g(x_1, \cdots, x_n = 0\)
Pour prendre en compte cette contrainte, on introduit une nouvelle fonction, \(F = f \pm \lambda g\), \(\lambda\) paramètre de Lagrange.

\(dF = 0 \Rightarrow \dfrac{\partial f}{\partial x_j} \pm \lambda \dfrac{\partial g}{\partial x_j}\) avec \(j = 1, \cdots, n\)

\subsection{Volume sphère de dimension n - Fonction \(\Gamma\)}

Le volume d'une hypersphère dans un espace à n dimensions est :
\[
	V_n = C_n r^n \text{ avec } C_n = \dfrac{\pi^{n/2}}{\Gamma\left( \frac{n}{2} + 1 \right)}
\]

La fonction \(\Gamma\) est telle que :
\begin{align*}
	\Gamma(x) &= \int_0^{+\infty} e^{-t} t^{x - 1} dt\\
	\Gamma (x+1) &= x \Gamma(x)\\
	\Gamma (\frac{1}{2}) &= \sqrt{\pi}\\
	\Gamma (1) &= 1	
\end{align*}
Si \(n \in N\) on a :
\[
\Gamma (n + 1) = n!
\]	


\chapter{Description statistique d'un système physique}

\begin{center}
\textbf{CM4 (2021-01-26)}
\end{center}

\section{Description quantique}

Un état quantique est définit par un ket : \(\vert \varphi \rangle\)
Son évolution est donné par l'équation de Schrödinger :
\[
	i\hbar \dfrac{\partial \vert \varphi \rangle}{\partial t} = \hat{H} \vert \varphi \rangle
\]

Avec H l'hamiltonien du système (opérateur associé à l'énergie)
\[
	\hat{H} = \hat{T} + \hat{V} = \dfrac{\hat{P}}{2m} + \hat{V}
\]

Si \(\hat{H}\) est indépendant du temps on a alors un système conservatif. les valeur propres de \(\hat{H}\)H donnes les valeurs possibles de l'énergie.

En général, les énergies sont dégénérées : plusieurs états peuvent avoir la même énergie.

Pour un état quantique macroscopique, on a un mélange statistique des états purs.
On a donc la valeur moyenne dans l'état pur \(\vert \varphi \rangle\) : \(\langle A_m \rangle = \langle \varphi_m \vert \hat{A} \vert \varphi_m \rangle\)

On aura à calculer des sommes sur les états stationnaires (\(\ell\)) de la forme :
\[
	\overline{A} = \sum_{(\ell)} P_{\ell} A_{\ell}
\]

Il ne faut pas confondre les niveaux d'énergie et les états stationnaires, en général, plusieurs états microscopisques (\(\ell\)) distincts correspondent à la même énergie

Mais la quantité à sommmer peut nedépendre que de l'énergie \(E_\ell\) de l'état (\(\ell\))

On peut remplacer \(\sum_{\text{état(}\ell)}\) par \(\sum_{\text{Niveau d'énergie}}\)

Pour un système macroscopique, l'écart entre 2 niveaux consécutifs d'énergie est très faible et on peut l'approximer comme continue pour pouvoir intégrer plutot que faire la somme discrète.

On définit la densité d'états \(dn(E)\) nombre d'état dont l'énergie est comprise entre E et E + dE.
On peut faire :
\[
	\overline{A} = \sum_{E_\ell} g(E_\ell)f(E_\ell) = \int_{E_0}^{\infty} \rho(E)f(E) dE
\]

Cette approximation n'est plus bonne si f(E) varie très brusquement.

En pratique il est plus facile de calculer le nombre d'état ne dépassant pas une certaine énergie \(\Phi(E)\), on a donc \(dn(E) = \Phi(E+dE) - \Phi(E)\) d'où comme \(dE \rightarrow 0\) : 
\[
	\rho(E) = \dfrac{d\Phi}{dE}
\]

\subsection{N particules dans une boite de volume V-Gaz parfait}

N particules sans interaction dans une boite de volume \( V \equiv GP\)

Énergies totale :
\[
	E = \sum_{i = 1}^N \varepsilon_i = (n_{x1}^2 + n_{y1}^2 + n_{z1}^2 + \cdots n_{xN}^2 + n_{yN}^2 + n_{zN}^2)\varepsilon_0
\]

Donc le nombre d'état à  une énergie inférieure à E est :
\[
	\Phi(E) = \dfrac{1}{2^{3N}} \dfrac{\pi^{3N/2}}{\Gamma(3N/2 + 1)} \left( \dfrac{E}{\varepsilon_0}\right)^{3N/2}
\]

La densité d'état est donc :
\[
	\rho(E) \propto E^{\frac{3N}{2} - 1}
\]

\subsection{Particules discernables - indiscernables}

En physique classiques les particules sont discernables car on peut suivre les trajectoires de chacunes d'entre elles.
En quantique, la notion de trajectoire n'existe pas.
Il est donc impossible de distingur deux quantons identiques indépendants, ils sont indiscernables.
En fais ce sont les positions occupé par les quantons qui seront discernables, ou pas.

La fonction d'onde d'un système de particule identique est symétrique ou antisymétrique par rapport à leur permutation

Si la fonction d'onde est symétrique on a des bosons.
Les bosons ont donc un spin entier : \(s = n \hbar \quad  n \in \ens{N}\)

Si la fonction est antisymétrique, on a des fermions, avec un spin demi-entier \(s = (n + \frac{1}{2}) \hbar \quad n \in \ens{N}\)

\begin{center}
\textbf{CM5 (2021-02-02)}
\end{center}

\section{Cas de particules discernables}

On a au total N particules et $n_r$ particules à un niveau d'énergie $\epsilon_r$ dégénéré $g_r$ fois.
Pour le $1^{\text{er}}$ niveau d'énergie on a $C_N^{n_1}$ façons de choisir $n_1$ particules parmi N.
Comme on a $g_1$ état de même énergie, il y a $g_1^{n_1}$ choix
\[
	\Omega_1 = C_N^{n_1} g_1^{n_1}
\]
Pour le $2^{\text{ème}}$ niveau, il ne reste que $N - n_1$ particules, on choisit $n_2$ parmi ces $N - n_1 \Rightarrow C_{N-n_1}^{n_2}$ choix possibles, il y a $g_2$ états $\Rightarrow g_2^{n_2}$.
etc... pour la suite :
\begin{align*}
	\Omega &= \prod_{i=1} \Omega_i\\
	&= \prod_{i=1} C_{N-\sum_{j=1}^{i-1} n_j}^{n_i} g_i^{n_i}\\
	&= \dfrac{N!}{n_1!(N-n_1)!} \times \dfrac{(N-n_1)!}{n_2!(N-n_1-n_2)!}\cdots \times g_1^{n_1} \cdot\\
	&= \dfrac{N!}{n_1!n_2! \cdots} g_1^{n_1}g_2^{n_2} \cdots\\
	&= N! \prod \dfrac{g_r^{n_r}}{n_r!}
\end{align*}


\subsection{Cas des fermions}

\subsection{Cas des bosons}

Un système est dilué si le nombre de particules $n_r$ à répartir dans les $g_r$ états est petit ($n_r \ll g_r$) alors on montre que :
\[
	\Omega^B \simeq \Omega^F \simeq \dfrac{1}{N!}\Omega^{\text{Discernable}}
\]

$\Rightarrow$ Boson et fermions dilués quasi équivalents aux particules classique mais avec un facteur d'indiscernabilité : $\frac{1}{N!}$

\section{Description d'un système classique}

\begin{center}
\textbf{CM5 (2021-02-08)}
\end{center}

\subsection{État microscopique - description classique}

La description quantique n'est pas toujours nécessaire. Le système est décrit par les positions et les impulsions des particules du système. Les lois de Newton décrivent les évolutions du système.

États du système est décrit par l'espace des phases, de dimension 6N, avec N le nombre de particules. La position en \(x\), \(y\) et \(z\) et l'impulsion en $p_x$, $p_y$ et $p_z$.
En mécanique classique le nombre d'état est infini (l'énergie n'est pas multiple entière d'un quantum d'énergie), les variables sont donc continues.

On quadrille l'espace en case de largeur $\delta_p$ et $\delta_q$, le volume d'une case est donc de \(h_0 = \delta_p \delta_q\) et son unité est le produit d'une longueur et d'une impulsion, son unité est donc des $J\cdot s$ (équivalent à l'unité de \(\hbar\).

Plus on prend une cellule petite, plus la précision est élevé. classiquement \(h_0\) peut-être aussi petit que l'on veut, mais la physique quantique impose \(h_0 \geq h\), la constante de Planck.

Si le nombre de paramètres nécessaire est grand, on utilise les probabilité pour définir un état macroscopique.
L'état macroscopique est donc caractérisé par une densité de probabilité dans l'espace des phases.
\(dP = w\left( q, p; t\right) dq~dp\) :Probabilité que le système se trouve dans l'état {q, p} à \(dq~dp\) près.

La valeur moyenne d'une grandeur physique f :
\[
	\overline{f} = \int f(q,p; t) w(q,p;t)dq dp
\]

L'approche classique est valable si l'action caractéristique du système est très supérieur à \(\hbar\) avec :\\
(action) \(\equiv\) (Énergie) \(\times\) (temps) ou (Longueur) \(\times\) (quantité de mouvement)

Pour un gaz parfait, on a la vitesse moyenne :
\[
	\overline{v} = \sqrt{\dfrac{3 k_B T}{m}}
\]

Avec, \(\ell\) la distance moyenne entre les particule :\(\ell = \left( \frac{V}{N}\right)^{1/3}\)

et, \(\lambda\) la longueur d'onde thermique de De Broglie : \(\lambda = \frac{\hbar}{\sqrt{3mk_BT}}\)

L'approche classique est valide si \(\ell \gg \lambda\), soit lorsque la température est élevé et/ou lorsque la densité est faible.

Si on impose des contraites au système, le volume accessible de l'espace des phases est limité à \(\mathfrak{V}_\phi\)
Alors le nombre de micro-états accessible au système est :
\[
	\dfrac{\mathfrak{V}_\phi}{h_0^n} \text{ avec } h_0 = h
\]

\paragraph{Exemple} Une particule m dans une boite de volume V


\(E = \left( p_x^2 + p_y^2 + p_z^2\right)/2m\)

\(\Phi(E)\) est le nombre d'états dont l'énergie est inférieure à E :\\
\(p_x^2 + p_y^2 + p_z^2 \leq 2mE\)
\[
	\Phi(E) = \dfrac{V \frac{4}{3} \pi (2mE)^{3/2}}{h^3}
\]

\chapter{Ensemble microcanonique}

\section{Équilibre}

Au niveau microscopique on a toujours des fluctuations, même à l'équilibre.

\paragraph{Définition} Ensemble statistique

Collection d'un grand nombre de système identique (réplique du système initial)

\paragraph{Définition} Moyenne d'ensemble

La moyenne d'ensemble d'une grandeur A est :
\[
	\overline{A} = \sum_{(\ell)} P_\ell A_\ell
\]

Tous les ensembles sont pris au même instant, mais en pratique on ne peut prendre qu'un seul système

\paragraph{Définition} Principe ergotique
On postule que dans un système à l'équilibre est :
\[
	\overline{A} = \sum_{(\ell)} P_\ell A_\ell = \underset{\tau \rightarrow \infty}{\lim} \dfrac{1}{\tau} \int_0^\tau A(t) dt = \langle A \rangle
\]

Principe essentiel en pratique : On mesure des moyennes temporelles mais la théorie est basée sur des moyennes d'ensemble.

\paragraph{Définition} Paramètres extérieurs

Propriétés fixées par des contraintes extérieures, valeurs statistiquement certaines même si il y a une incertitude expérimentale.

\paragraph{Définition} Variables internes

Propriétés libres de fluctuer au gré de l'agitation microscopique.

Selon les cas, une même grandeur peut être un parmètre extérieur ou une variable interne.

\paragraph{Définition} États accessibles

États microscopique vérifiant les contraintes extérieures, c'est-à-dire compatible avec les valeurs des paramètres extérieurs.

\paragraph{Postulat}

Pour un système isolé à l'équilibre macroscopique, tous les états accessibles ont la même probabilité.

Soit E l'énergies du système (fixée) et \(\delta E\) l'incertitude macroscopique associée.

Les états microscopique accessibles (\(\ell\)) sont ceux qui vérifient toutes les autres contraintes extérieures.

L'ensemble constitué de systèmes isolés identique est appelé ensemble microcanonique. Dans cet ensemble seuls apparaissent les états (\(\ell\)) accessibles et ils apparaissent avec la même fréquence dnas la limite \(\mathcal{N} \rightarrow \infty\), postulat fondamental \(\equiv\) principe du "désordre maximum" :

La distribution microcanonique est celle qui maximise l'entropie statistique (On parle de désordre mais ça représente plutot le manque d'information).

On associe une entropie appelée entropie microcanonique à la distribution microcanonique

\paragraph{Définition} Entropie microcanonique :

Les micro-états étant équiprobables, l'entropie microcanonique est :
\[
	S^* = k_B \ln \Omega
\]
Avec \(\Omega\) le nombre d'états accessibles.

\(\Omega(E,x, \delta	E )= \rho(E, x)\delta E \)

\(\ln \Omega = \ln \rho(E,x) + \ln \delta E\)

\(S^* (E,x) = k_B \ln \rho(E,x)\) et aussi \(S^* (E,x) = k_B \ln \Phi(E)\)

\begin{center}
\textbf{CM6 (2021-02-09)}
\end{center}

L'entropie \(S^\star\) est une valeur extensive

\paragraph{Temperature \(T^\star\)}

Définie par 
\[
	\dfrac{1}{T^\star} = \left( \dfrac{\partial S^\star}{\partial E}\right)_{V,N}
\]

\paragraph{Pression \(P^\star\)}

Définie par
\[
	\dfrac{p^\star}{T^\star} = \left( \dfrac{\partial S^\star}{\partial V}\right)_{E,N}
\]

\paragraph{Potentiel chimique \(\mu^\star\)}

Définie par
\[
	\dfrac{\mu^\star}{T^\star} = \left( \dfrac{\partial S^\star}{\partial N}\right)_{E,V}
\]

On a donc :
\[
	dS^\star = \dfrac{dE}{T^\star} + \dfrac{p^\star}{t^\star}dV + \dfrac{\mu^\star}{T^\star}dN
\]

On s'intéresse aux changements d'états d'équilibre d'un système isolé.

On prend un gaz isolé dans un volume V, on ouvre une enceinte vide de volume \(\Delta V\), plus d'état son possible, et donc l'entropie va augmenter.

La modification de contrainte extérieures augmente l'entropie et le prochain état d'équilibre correspond au nouveau maximum de son entropie.

\paragraph{Exemple}

Volume séparé en deux parti séparé par une paroi isolante qui devient diatherme. À l'équilibre final on aura l'état à l'entropie maximal, une température égal dans les deux parties du volume.

L'énergie du système est de \(E = E_1 + E_2\)

Initialement \(E_1\) et \(E_2\) sont des paramètres extérieurs mais après changement de la paroi, \(E_1\) et \(E_2\) deviennent des variables internes

\[
	w(E_1) = \dfrac{\omega(E,V,N,E_1)\delta E_1}{\Omega(E,V,N)}
\]
Avec \(\omega(E,V,N,E_1)\), le nombre d'états accessible au systeme tels que l'énergie de la zone 1 soit égale à \(E_1\) (à \(\delta E_1\) près) et \(\Omega(E,V,N)\) le nombre d'états accessibles.

À l'équilibre \(\tilde{E_1}\) maximise \(s^\star(E_1) = S_1^\star(E_1) + S_2^\star(E-E_1)\)

\[
	\dfrac{\partial s^\star}{\partial E_1}(E_1 = \tilde{E_1} = 0 = \dfrac{\partial S_1^\star}{\partial E_1} + \dfrac{\partial S_2^\star}{\partial E_2} \dfrac{\partial E_2}{\partial E_1}
\]
On a \(E_2 = E-E_1\), donc :
\[
	\dfrac{\partial E_2}{\partial E_1} = -1 \text{ et } \dfrac{\partial S_1^\star}{\partial E_1} (E_1 = \tilde{E_1}) - \dfrac{\partial S_2^\star}{\partial E_2} (E_2 = E - \tilde{E_1}) = 0
\]
\[
	\Leftrightarrow \dfrac{1}{T_1^\star} = \dfrac{1}{T_2^\star}
\]
\[
	\Leftrightarrow T_1^\star = T_2^\star
\]

\begin{center}
\textbf{CM7 (2021-02-**)}
\end{center}

\end{document}