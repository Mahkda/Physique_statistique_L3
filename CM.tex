\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\author{Malo Kerebel}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black, urlcolor=black
}

%Macro
\newcommand{\ens}[1]{\mathbb{#1}}

\begin{document}


\begin{titlepage}

\centering{
	
	{\scshape\LARGE Université de Bretagne Occidentale \par}
	\vspace{1cm}
	{\scshape\Large Note de cours\par}
	\vspace{1.5cm}
	{\huge\bfseries Physique statistique\unskip\strut\par}
	\vspace{2cm}
	{\Large\itshape Malo Kerebel \par}
	\vfill
	Cours par\par
	Jean-Philippe \textsc{Jay}

	\vfill

% Bottom of the page
	{\large Semestre 6, année 2020-2021 \par}
}
\end{titlepage}

\tableofcontents

\chapter{Introduction générale}
\begin{center}
\textbf{CM1 (2021-01-12)}
\end{center}

physique statistique : étude du mouvement de gaz à l'échelle macroscopique.
Différences entre fermion et boson : leur spin, fermion spin demi entier (eg. les électrons), boson particule à spin entier (eg photon)\par
\quad \par
Bibliographie:
Physique Statistique, B. Diu, C. uthmann, D.Lederer\\
Physique Statistique, H. Ngp, C. Ngo\par
\quad \par

\section{Buts de la physique statistique}
Unifier le macroscopique et le microscopique, au $\text{XIV}^{\text{\`eme}}$ siècle, on a la thermodynamique, la mécanique et l'électro-magnétisme mais rien qui relie les uns aux autres.

Au niveau microscopique on a $\approx 10^{23}$ paramètres (de l'ordre du nombre d'Avogadro), auquel il faut avoir la vitesse et la position, il est impossible d'appliquer les résultats de la mécanique macroscopique dessus.
La physique statistique a donc pour but d'expliquer les comportements collectifs, de particules mais les résultats peuvent s'étendre à des réseaux de neurones ou des comportements de foules.

\section{Combinatoire}
Le dénombrement des objets ou des configuration. 2 système indépendant A et B, ayant $\Omega_a$ et $\Omega_b$ configurations, il y a \(\Omega_a \times \Omega_b\) configurations possible pour la juxtaposition de A et B.

\begin{center}
\textbf{CM2 (2021-01-14)}
\end{center}

Le nombre de permutations de N objets parmi N est N!

Le nombre de combinaison sans répétition s'obtient avec :
\[
	C^k_n = \dfrac{n!}{k!(n-k)!}
\]

\section{Probabilités - Statistiques}

événement aléatoire : résultat possible d'une expérience

Variable aléatoire : variable qui peut prendre l'une quelconque de ses valeurs possibles, inconnue d'avance. Discrète il y a un nombre finie de valeur, continue il y a un nombre infinie de valeur possible

\paragraph{Propriétés}
\begin{enumerate}
	\item \(0 \leq P_m \leq 1 \quad \forall m\)
	\item \(\sum_m P_m = 1\) (normalisation)
\end{enumerate}

Pour une variable aléatoire continue on utilise la densité de probabilité, qu'une la variable \(\in [x, x +\delta x]\)
\[
	w(x) = \lim_{\delta_x \rightarrow 0^+} \frac{\delta P(x)}{\delta_x}
\]
\[
	dP(x) = w(x)dx = \lim_{N \rightarrow \infty} \frac{dN(x)}{N}
\]
De même il y a la normalisation :
\[
	\int_{-\infty}^{+\infty} w(x)dx = 1
\]

Quand on est dans les bonne conditions :
\[
	P(e_1 ou e_2) = P(e_1) + P(e_2)
\]
\[
	P(e_1 et e_2) = P(e_1)\cdot P(e_2)
\]

L'écart quadratique moyen, ou variance, caratérise la dispersion de la distribution statistique, il est défini par :
\[
	(\Delta_f)^2 = \overline{(f - \overline{f})^2} = \overline{f^2} - (\overline{f})^2
\]
De même on définit l'écart-type \(\sigma\) comme la racine carré de la variance :
\[
	\sigma = \sqrt{(\Delta_f)^2} = \Delta_f
\]

La moyenne de résultat est :
\[
	\overline{n} = pN
\]

\begin{center}
\textbf{CM3 (2021-01-19)}
\end{center}

Après calculs : \(\overline{n^2} = pN + p^2N(N-1)\)
Donc \(\sigma^2 = \overline{n^2} - (\overline{n})^2 = pN + p^2N^2 - p^N - p^2N^2 = pN - p^2N\)
\[
	\sigma^2 = N p q = Np(1-p)
\]

Ainsi on voit que l'écart type croit avec N :
(Cependant, la dispersion relative à la moyenne décroit)
\[
	\dfrac{\sigma}{\overline{n}} \sim \dfrac{\sqrt{N}}{N} = \dfrac{1}{\sqrt{N}}
\]

\subsection{Loi de Poisson}

Hypothèses :
\begin{itemize}
	\item[•] \(p \rightarrow 0\)
	\item[•] \(N \rightarrow +\infty\)
	\item[•] MAIS \(pN = \lambda\) reste une valeur finie.
\end{itemize}

La loi de poisson dit donc que la loi binomiale devient :
\[
	P(N, n) = \dfrac{\lambda^n}{n!}e^{-\lambda}
\]
La proba ne dépend pas du nombre d'expériences N
Exercice : Vérifier que c'est bien normalisé.
\[
	\sum_n P(N,n) = 1 = \sum_n \dfrac{\lambda^n}{n!}e^{-\lambda} = e^\lambda e^{-\lambda} = 1
\]

On a :
\[
	\overline{n} = \sigma^2 = \lambda
\]
On peut donc aussi écrire la loi de poisson sous la forme :
\[
	P = \dfrac{\overline{n}^n}{n!}e^{-\overline{n}}
\]

\subsection{Loi Gaussienne ou loi normale}

La densité de probabilité est :
\[
	w_G (c) = \dfrac{1}{\sqrt{2\pi \sigma^2}} e^{\left[ \dfrac{- (x-x_0)^2}{2\sigma^2} \right]}
\]
\(w_G\) dépend de 2 paramètres, le centre (ou la moyenne) \(x_0\) et la largeur \(\sigma\)

\subsection{Théorème de la limite centrale}

La distribution de la somme de N variables aléatoires indépendantes et de même loi devient gaussienne lorsque N devient grand

En particulier la loi binomiale \(\rightarrow\) loi gaussienne

Théorème important en physique stats, car les systèmes macroscopiques sont souvent constitué d'un très grand nombre d'éléments faiblement couplés.

Les variables libres de fluctuer ont alors une distribution gaussienne.

\section{Entropie statistique}

Les informations concernant le ssystème physique est incomplète
L'entropie statistique mesure le manque d'information

\subsection{Définition de l'entropie statistique}

Ensemble fini d'événements \(e_m (m = 1,2, \cdots , M)\) de proba \(P_m\).
Tout autre distribution implique un manque d'information
ENtropie statistique :
\[
	S(P_1, P_2, \cdots, P_M) = -k \sum_{m=1}^M P_m \ln(P_m)
\]
Avec \(P_m \ln(P_m) = 0\) si \(P_m = 0\) et \(k = k_B = 1,380649 \times 10^{-23} J\cdot K^{-1}\) la constante de Boltzmann, seule l'ordre de grandeur et l'unité sont à retenir.

L'entropie est nulle si une des proba des évenement possible est de 1. Inversement l'entropie est maximale quand tous les évenements sont équiprobable. L'entropie augmente avec le nombre d'évenement possible.

\section{Quelques résultat de maths}

\subsection{Approximation de Stirling}

Approximation utile pour plus tard :
Pour \(N \gg 1\), on a :
\[
	\ln(N!) \sim N \ln(N) - N
\]

\subsection{Intégrales gaussiennes}

\[
	\int_{-\infty}^{+\infty} e^{-\alpha x^2}dx = \sqrt{\dfrac{\pi}{\alpha}}
\]

\subsection{Méthode des multiplicateurs de Lagrange}

f fonction de n variables indépendantes. Il y a un maxima si :
\[
	\dfrac{\partial f}{\partial x_1} = \cdots = \dfrac{\partial f}{\partial x_n} = 0
\]

Si il y a une contraintes entre les variables, ex \(g(x_1, \cdots, x_n = 0\)
Pour prendre en compte cette contrainte, on introduit une nouvelle fonction, \(F = f \pm \lambda g\), \(\lambda\) paramètre de Lagrange.

\(dF = 0 \Rightarrow \dfrac{\partial f}{\partial x_j} \pm \lambda \dfrac{\partial g}{\partial x_j}\) avec \(j = 1, \cdots, n\)

\subsection{Volume sphère de dimension n - Fonction \(\Gamma\)}

Le volume d'une hypersphère dans un espace à n dimensions est :
\[
	V_n = C_n r^n \text{ avec } C_n = \dfrac{\pi^{n/2}}{\Gamma\left( \frac{n}{2} + 1 \right)}
\]

La fonction \(\Gamma\) est telle que :
\begin{align*}
	\Gamma(x) &= \int_0^{+\infty} e^{-t} t^{x - 1} dt\\
	\Gamma (x+1) &= x \Gamma(x)\\
	\Gamma (\frac{1}{2}) &= \sqrt{\pi}\\
	\Gamma (1) &= 1	
\end{align*}
Si \(n \in N\) on a :
\[
\Gamma (n + 1) = n!
\]	


\chapter{Description statistique d'un système physique}

\begin{center}
\textbf{CM4 (2021-01-26)}
\end{center}

\section{Description quantique}

Un état quantique est définit par un ket : \(\vert \varphi \rangle\)
Son évolution est donné par l'équation de Schrödinger :
\[
	i\hbar \dfrac{\partial \vert \varphi \rangle}{\partial t} = \hat{H} \vert \varphi \rangle
\]

Avec H l'hamiltonien du système (opérateur associé à l'énergie)
\[
	\hat{H} = \hat{T} + \hat{V} = \dfrac{\hat{P}}{2m} + \hat{V}
\]

Si \(\hat{H}\) est indépendant du temps on a alors un système conservatif. les valeur propres de \(\hat{H}\)H donnes les valeurs possibles de l'énergie.

En général, les énergies sont dégénérées : plusieurs états peuvent avoir la même énergie.

Pour un état quantique macroscopique, on a un mélange statistique des états purs.
On a donc la valeur moyenne dans l'état pur \(\vert \varphi \rangle\) : \(\langle A_m \rangle = \langle \varphi_m \vert \hat{A} \vert \varphi_m \rangle\)

On aura à calculer des sommes sur les états stationnaires (\(\ell\)) de la forme :
\[
	\overline{A} = \sum_{(\ell)} P_{\ell} A_{\ell}
\]

Il ne faut pas confondre les niveaux d'énergie et les états stationnaires, en général, plusieurs états microscopisques (\(\ell\)) distincts correspondent à la même énergie

Mais la quantité à sommmer peut nedépendre que de l'énergie \(E_\ell\) de l'état (\(\ell\))

On peut remplacer \(\sum_{\text{état(}\ell)}\) par \(\sum_{\text{Niveau d'énergie}}\)

Pour un système macroscopique, l'écart entre 2 niveaux consécutifs d'énergie est très faible et on peut l'approximer comme continue pour pouvoir intégrer plutot que faire la somme discrète.

On définit la densité d'états \(dn(E)\) nombre d'état dont l'énergie est comprise entre E et E + dE.
On peut faire :
\[
	\overline{A} = \sum_{E_\ell} g(E_\ell)f(E_\ell) = \int_{E_0}^{\infty} \rho(E)f(E) dE
\]

Cette approximation n'est plus bonne si f(E) varie très brusquement.

En pratique il est plus facile de calculer le nombre d'état ne dépassant pas une certaine énergie \(\Phi(E)\), on a donc \(dn(E) = \Phi(E+dE) - \Phi(E)\) d'où comme \(dE \rightarrow 0\) : 
\[
	\rho(E) = \dfrac{d\Phi}{dE}
\]

\subsection{N particules dans une boite de volume V-Gaz parfait}

N particules sans interaction dans une boite de volume \( V \equiv GP\)

Énergies totale :
\[
	E = \sum_{i = 1}^N \varepsilon_i = (n_{x1}^2 + n_{y1}^2 + n_{z1}^2 + \cdots n_{xN}^2 + n_{yN}^2 + n_{zN}^2)\varepsilon_0
\]

Donc le nombre d'état à  une énergie inférieure à E est :
\[
	\Phi(E) = \dfrac{1}{2^{3N}} \dfrac{\pi^{3N/2}}{\Gamma(3N/2 + 1)} \left( \dfrac{E}{\varepsilon_0}\right)^{3N/2}
\]

La densité d'état est donc :
\[
	\rho(E) \propto E^{\frac{3N}{2} - 1}
\]

\subsection{Particules discernables - indiscernables}

En physique classiques les particules sont discernables car on peut suivre les trajectoires de chacunes d'entre elles.
En quantique, la notion de trajectoire n'existe pas.
Il est donc impossible de distingur deux quantons identiques indépendants, ils sont indiscernables.
En fais ce sont les positions occupé par les quantons qui seront discernables, ou pas.

La fonction d'onde d'un système de particule identique est symétrique ou antisymétrique par rapport à leur permutation

Si la fonction d'onde est symétrique on a des bosons.
Les bosons ont donc un spin entier : \(s = n \hbar \quad  n \in \ens{N}\)

Si la fonction est antisymétrique, on a des fermions, avec un spin demi-entier \(s = (n + \frac{1}{2}) \hbar \quad n \in \ens{N}\)

\begin{center}
\textbf{CM4 (2021-02-02)}
\end{center}

\section{Cas de particules discernables}

On a au total N particules et $n_r$ particules à un niveau d'énergie $\epsilon_r$ dégénéré $g_r$ fois.
Pour le $1^{\text{er}}$ niveau d'énergie on a $C_N^{n_1}$ façons de choisir $n_1$ particules parmi N.
Comme on a $g_1$ état de même énergie, il y a $g_1^{n_1}$ choix
\[
	\Omega_1 = C_N^{n_1} g_1^{n_1}
\]
Pour le $2^{\text{ème}}$ niveau, il ne reste que $N - n_1$ particules, on choisit $n_2$ parmi ces $N - n_1 \Rightarrow C_{N-n_1}^{n_2}$ choix possibles, il y a $g_2$ états $\Rightarrow g_2^{n_2}$.
etc... pour la suite :
\begin{align*}
	\Omega &= \prod_{i=1} \Omega_i\\
	&= \prod_{i=1} C_{N-\sum_{j=1}^{i-1} n_j}^{n_i} g_i^{n_i}\\
	&= \dfrac{N!}{n_1!(N-n_1)!} \times \dfrac{(N-n_1)!}{n_2!(N-n_1-n_2)!}\cdots \times g_1^{n_1} \cdot\\
	&= \dfrac{N!}{n_1!n_2! \cdots} g_1^{n_1}g_2^{n_2} \cdots\\
	&= N! \prod \dfrac{g_r^{n_r}}{n_r!}
\end{align*}


\section{Cas des fermions}

\section{Cas des bosons}

Un système est dilué si le nombre de particules $n_r$ à répartir dans les $g_r$ états est petit ($n_r \ll g_r$) alors on montre que :
\[
	\Omega^B \simeq \Omega^F \simeq \dfrac{1}{N!}\Omega^{\text{Discernable}}
\]

$\Rightarrow$ Boson et fermions dilués quasi équivalents aux particules classique mais avec un facteur d'indiscernabilité : $\frac{1}{N!}$
\end{document}